######################################
#
# CONFIG TEMPLATES
# (actual configuration is at the end)
#

# SERVER CONFIGS

generic-api = {
  # The host for a service only needs to be specified by client connections; the
  # server itself will determine its hostname when it is started.
  host = localhost

  # The port that the service is listening on.
  port = null
  # The port that the service is listening on when using HTTPS
  secure-port = null

  # User name for connecting to the service, if required.
  user-name = null
  # Password for connecting to the service, if required.
  password = null

  # The keystore file containing the server's SSL keys. Only necessary when
  # accessing a server with temporary self-signed key, which is not trusted
  # by the Java SSL implementation.
  keystore-file = null
  keystore-password = null
}


serving-layer = {
  # Default connection settings for the Oryx model serving layer.
  api = ${generic-api} {
    port = 80
    secure-port = 443

    # If true, operations that set or modify data are not available
    read-only = false

    # An optional prefix for the path under which the service is deployed. For
    # example, "http://example.org/contextPath/..."
    context-path = /
  }

  # Optional. The local directory used for reading input, writing output, and storing
  # user input and model files in local mode. It is used for staging input for upload in distributed mode.
  local-input-dir = /tmp/Oryx

  # Optional. Name of an implementation of RescorerProvider to use to rescore recommendations and similarities,
  # if any. The class must be added to the server classpath. This may also be specified as a comma-separated
  # list of class names, in which case all will be applied, in the given order.
  rescorer-provider-class = null

  candidate-filter-class = null
}


computation-layer = {
  # Default connection settings for the Oryx model computation layer.
  api = ${generic-api} {
    port = 8080
    secure-port = 8443
  }

  # Max degree of parallelism (really, number of reducers or threads)
  # "auto" to try to guess the best value
  parallelism = "auto"

  # Memory requested for mappers' and reducers' Hadoop containers
  mapper-memory-mb = 1024
  reducer-memory-mb = 1024

  # Multiple of memory required per worker (really, reducer), in MB, for high-memory steps
  # Default to 3x the M/R default of 1024
  worker-high-memory-factor = 3
}


# MODEL CONFIGS

# Parameters common to all models

model = {

  # Whether to run the model locally without Hadoop
  local = true

  # What type of model is this
  type = null

  # Time between rebuilds, in minutes
  time-threshold = -1

  # Data written between rebuilds, in MB
  data-threshold = -1

  # Number of writes between rebuilds -- only local mode
  writes-between-rebuild = 100000

  # Number of writes between uploads to HDFS -- only distributed mode
  writes-between-upload = 10000

  generations = {
    # Number of generations to keep
    keep = 10
  }

  # Location where all data and work occurs
  instance-dir = null

}


# Only one of the following *-model values should be configured.
# Specifically, assign only one *-model object to model below

# Model for alternating-least-squares-based recommender
als-model = ${model} {

  type = als

  # Default number of model features
  features = 30
  # Default over-fitting parameter, lambda
  lambda = 0.1
  # Default alpha model parameter
  alpha = 1.0

  iterations = {
    # Maximum number of iterations to run, regardless of convergence
    max = 30
    # If average absolute change in reconstructed values is below this, stop iterating
    convergence-threshold = 0.001
  }

  recommend = {
    # If true, compute recs for all users too
    compute = false
    how-many = 10
  }

  item-similarity = {
    # If true, compute most-similar items for all items
    compute = false
    how-many = 10
  }

  # If true, don't use values as weights, but actually try to reconstruct input values
  # (Reconstruct R matrix in the algorithm, not P
  # Don't set this in general.
  reconstruct-r-matrix = false

  # If true, change algorithm's loss function to only include elements that are present in the input
  # That is, don't penalize not reconstructing the implicit 0 elements.
  # Don't set this in general.
  loss-ignores-unspecified = false

  # Controls whether model data 'decays' with each generation. Does not affect local computation.
  decay = {
    # New value as decayed fraction of old value; in (0,1]
    factor = 1.0
    # Value below which something is considered to be effectively 0
    zeroThreshold = 0.0
  }

  # Configures location-sensitive hashing
  lsh = {
    # Target fraction of total elements to consider by sampling
    sample-ratio = 1.0
    # Number of bits of hash to use
    num-hashes = 20
  }

  # If true, don't remember items associated to each user. Saves memory; recommendations will have
  # items already interacted with though
  no-known-items = false

}


kmeans-model = ${model} {

  type = kmeans

  normalize = {
    default-transform = none
    #sparse = true
    #z-transform = ["foo", 7 ]
    #no-transform = [ "abc" ]
    #linear-transform = [ 2, 3, 4 ]
  }

  # How many sketch passes to run
  sketch-iterations = 5
  # How many points to sample on each iteration
  sketch-points = 100

  # How many cross-folds to use during the evaluation of K; two is almost always the right choice.
  cross-folds = 2

  # For the LSH index, how many bits to track. Increasing this number increases the runtime of the
  # iterations but improves the accuracy of the clustering.
  index-bits = 128
  # For the LSH index, the number of points from the index to consider for each point. Increasing this
  # number increases the runtime of the iterations, but improves the accuracy of the clustering.
  index-samples = 32

  # Candidate values of K to consider. K = 1 is always included by default for evaluation purposes.
  k = [2, 5, 10]
  # How many random re-starts to perform for each value of K.
  replications = 3
  # The number of reducers to use to run the k-means computations, which can be very computationally intensive.
  parallelism = 3

  init-strategy = "PLUS_PLUS" # One of PLUS_PLUS or RANDOM
  update-strategy = {
    type = MINIBATCH # One of LLOYDS or MINIBATCH
    iterations = 100 # Iterations for the strategy (required for both LLOYDS and MINIBATCH)
    batch-size = 100 # Only used by MINIBATCH: how many points to include in each mini-batch run
  }
  eval-strategy {
    type = THRESHOLD # One of THRESHOLD or FIXED
    threshold = 0.5  # Find the best cost solution that had a Variation of Information < threshold
    #k = 10          # For type = FIXED, choose the lowest cost solution for some fixed k
  }
  outliers = {
    compute = true
    mahalanobis = true
  }
}

rdf-model = ${model} {

  type = rdf

  # Number of trees to build
  num-trees = 20
  # Fraction of features to consider splitting on at each node
  fraction-features-to-try = 0.5
  # Only split nodes when the resulting children have at least this many examples:
  min-node-size = 2
  # Only split nodes when a split can be found that gains at least this much information, in nats (not bits)
  min-info-gain-nats = 0.0001
  # How much of the data should each tree learn on, approximately:
  sample-rate = 1.0
  # Maximum number of splits, per feature, to consider at a node.
  max-split-candidates = 8
  # Maximum tree depth
  max-depth = 64

}

# INBOUND DATA CONFIGS

# Information for parsing inbound data.
csv-input = {
  type = csv
  # The delimiter for all delimited input.
  delim = ","

  #column-names = [ "foo", "bar", "baz" ]
  column-names = []

  #id-columns = ["foo", 2, 7] # Valid syntax
  #ignored-columns = [ "foo", 1 ] # Valid syntax
  id-columns = []
  ignored-columns = []

  #Set one of these two:
  #numeric-columns = [ 2, "baz" ] # Valid syntax
  #categorical-columns = [ 1 ]

  target-column = null
}

# hive-input = { # TODO }

inbound = ${csv-input}
