/*
 * Copyright (c) 2013, Cloudera, Inc. All Rights Reserved.
 *
 * Cloudera, Inc. licenses this file to you under the Apache License,
 * Version 2.0 (the "License"). You may not use this file except in
 * compliance with the License. You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * This software is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR
 * CONDITIONS OF ANY KIND, either express or implied. See the License for
 * the specific language governing permissions and limitations under the
 * License.
 */

package com.cloudera.oryx.common.servcomp;

import java.io.File;
import java.net.MalformedURLException;
import java.util.Arrays;
import java.util.Collection;
import java.util.Collections;
import java.util.Iterator;
import java.util.List;

import com.google.common.base.Joiner;
import com.google.common.base.Preconditions;
import com.google.common.base.Splitter;
import com.google.common.collect.Lists;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.CommonConfigurationKeysPublic;

import com.cloudera.oryx.common.settings.ConfigUtils;

/**
 * {@link Configuration} subclass which manages app-specific settings. Subclasses ensure that settings
 * specific to the underlying datastore are set as well.
 * 
 * @author Sean Owen
 */
public final class OryxConfiguration extends Configuration {

  private static final String HADOOP_CONF_DIR_KEY = "HADOOP_CONF_DIR";
  private static final String DEFAULT_HADOOP_CONF_DIR = "/etc/hadoop/conf";

  public OryxConfiguration() {
    init();
  }

  public OryxConfiguration(Configuration configuration) {
    super(configuration);
    init();
  }

  private void init() {
    if (!ConfigUtils.getDefaultConfig().getBoolean("model.local")) {
      File hadoopConfDir = findHadoopConfDir();
      addResource(hadoopConfDir, "hdfs-default.xml");
      addResource(hadoopConfDir, "hdfs-site.xml");
      addResource(hadoopConfDir, "mapred-default.xml");
      addResource(hadoopConfDir, "mapred-site.xml");
      addResource(hadoopConfDir, "yarn-default.xml");
      addResource(hadoopConfDir, "yarn-site.xml");
      addResource(hadoopConfDir, "core-site.xml");

      // Can we do away with these? the standard config generated by CDH4 still refers to old prop names
      setFromEnvOrConf(Collections.singleton(Arrays.asList(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY,
                                                           "fs.default.name")));

      fixLzoCodecIssue();
    }
  }

  private void addResource(File hadoopConfDir, String fileName) {
    File file = new File(hadoopConfDir, fileName);
    if (!file.exists()) {
      return;
    }
    try {
      addResource(file.toURI().toURL());
    } catch (MalformedURLException e) {
      throw new IllegalStateException(e);
    }
  }

  /**
   * @param keySynonymGroupsToSet several groups of keys whose values should be set. Within each group, if a
   *  value is found for any key, it is set to the first keys in the group, which is considered the definitive
   *  value. This is repeated for each group.
   */
  private void setFromEnvOrConf(Collection<? extends Collection<String>> keySynonymGroupsToSet) {
    Preconditions.checkArgument(!keySynonymGroupsToSet.isEmpty());
    for (Collection<String> synonymGroup : keySynonymGroupsToSet) {
      String value = null;
      for (String synonym : synonymGroup) {
        value = get(synonym);
        if (value != null) {
          break;
        }
      }
      Preconditions.checkNotNull(value, "None of %s specified, and not set", synonymGroup);
      set(synonymGroup.iterator().next(), value);
    }
  }

  private static File findHadoopConfDir() {
    String hadoopConfPath = System.getenv(HADOOP_CONF_DIR_KEY);
    if (hadoopConfPath == null) {
      hadoopConfPath = DEFAULT_HADOOP_CONF_DIR;
    }
    File hadoopConfDir = new File(hadoopConfPath);
    Preconditions.checkState(hadoopConfDir.exists() && hadoopConfDir.isDirectory(),
                             "Not a directory: %s", hadoopConfDir);
    return hadoopConfDir;
  }

  /**
   * Removes LzoCodec and LzopCodec from io.compression.codecs. Implementations aren't shipped
   * with Hadoop, but are in some cases instantiated anyway even when unused. So, try
   * to erase them.
   */
  private void fixLzoCodecIssue() {
    String codecsProperty = get("io.compression.codecs");
    if (codecsProperty != null && !codecsProperty.isEmpty()) {
      List<String> codecs = Lists.newArrayList(Splitter.on(',').split(codecsProperty));
      for (Iterator<String> it = codecs.iterator(); it.hasNext(); ) {
        if (it.next().contains(".lzo.Lzo")) {
          it.remove();
        }
      }
      set("io.compression.codecs", Joiner.on(',').join(codecs));
    }
  }

}
